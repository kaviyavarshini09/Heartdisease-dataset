# Heartdisease-dataset
# ğŸ§  Task 5: Decision Trees and Random Forests - AI & ML Internship

## ğŸ“Œ Objective
To explore and implement tree-based models such as **Decision Tree Classifier** and **Random Forest Classifier** on the Heart Disease dataset. The goal is to learn model training, visualization, evaluation, and feature importance interpretation.

---

## ğŸ—ƒï¸ Dataset
**Heart Disease Dataset** from Kaggle:  
ğŸ”— https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset

---

## ğŸ› ï¸ Tools & Libraries Used
- Python
- Pandas & NumPy
- Scikit-learn
- Matplotlib & Seaborn
- Graphviz (for decision tree visualization)

---

## ğŸš€ Steps Performed

### âœ… Data Preprocessing
- Loaded the dataset and explored features.
- Separated features (X) and target (y).
- Scaled features using `StandardScaler`.

### âœ… Model Training
- Trained a **Decision Tree Classifier** with 'max_depth=4'.
- Trained a **Random Forest Classifier** with 100 estimators.

### âœ… Visualization
- Visualized the trained Decision Tree using 'plot_tree()'.
- Plotted **feature importances** using Random Forest output.

### âœ… Evaluation
- Calculated accuracy for both models.
- Compared performance of Decision Tree vs Random Forest.
- Used 5-fold **Cross-Validation** for model stability.

---

## ğŸ“Š Results

| Model              | Accuracy (Test) |
|--------------------|-----------------|
| Decision Tree      |  **~83%**       |
| Random Forest      |  **~88%**       |
| Cross-Val (Random) |  **~85â€“90%**    |

---

## ğŸ” Key Insights
- **Random Forest** performed better than a single decision tree.
- Limiting 'max_depth' helped reduce overfitting.
- Top predictive features were: 'cp', 'thal', and 'ca'.
- Ensemble methods like bagging increase model stability and accuracy.

---
